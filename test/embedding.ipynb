{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T03:24:57.639061Z",
     "start_time": "2025-07-25T03:24:52.105677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from modelscope import AutoModel\n",
    "from PIL import Image\n",
    "\n",
    "MODEL_NAME= \"/home/public/dkx/model/BAAI/BGE-VL-v1.5-zs\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model.eval()\n",
    "model.cuda()"
   ],
   "id": "aec64a6b3c13f922",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9889336aebc640119083489ea99d0e3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LLaVANextForEmbedding(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaNextMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T03:54:54.703717Z",
     "start_time": "2025-07-25T03:54:54.498113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    model.set_processor(MODEL_NAME)\n",
    "\n",
    "    # query_inputs = model.data_process(\n",
    "    #     text=\"Make the background dark, as if the camera has taken the photo at night\",\n",
    "    #     images=\"./assets/cir_query.png\",\n",
    "    #     q_or_c=\"q\",\n",
    "    #     task_instruction=\"Retrieve the target image that best meets the combined criteria by using both the provided image and the image retrieval instructions: \"\n",
    "    # )\n",
    "\n",
    "    candidate_inputs = model.data_process(\n",
    "        text=\n",
    "\"\"\"\n",
    "{\"id\":10093,\"image_url\":\"/home/dkx/RAG4Ghart/bar/10093.png\",\"type\":\"bar\",\"theme\":\"Science and Engineering\",\"title\":\"Data on R&D investment across cities\",\"distribution\":\"random\",\"display\":\"horizontal\",\"header\":\"City,million USD(million USD)\",\"data\":[{\"Crop category\":\"Rapeseed\",\"metric tons\":20000},{\"Crop category\":\"Wheat\",\"metric tons\":2694},{\"Crop category\":\"Apple\",\"metric tons\":2488},{\"Crop category\":\"Sugarcane\",\"metric tons\":7573}]}\n",
    "\"\"\"\n",
    "        ,\n",
    "        images=[\"\"],\n",
    "        q_or_c=\"c\",\n",
    "    )\n",
    "\n",
    "    # query_embs = model(**query_inputs, output_hidden_states=True)[:, -1, :]\n",
    "    candi_embs = model(**candidate_inputs, output_hidden_states=True)[:, -1, :]\n",
    "\n",
    "    # query_embs = torch.nn.functional.normalize(query_embs, dim=-1)\n",
    "    candi_embs = torch.nn.functional.normalize(candi_embs, dim=-1)\n",
    "\n",
    "    # scores = torch.matmul(query_embs, candi_embs.T)\n",
    "print(candi_embs.shape)"
   ],
   "id": "557704041e9d71ad",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      2\u001B[39m     model.set_processor(MODEL_NAME)\n\u001B[32m      4\u001B[39m     \u001B[38;5;66;03m# query_inputs = model.data_process(\u001B[39;00m\n\u001B[32m      5\u001B[39m     \u001B[38;5;66;03m#     text=\"Make the background dark, as if the camera has taken the photo at night\",\u001B[39;00m\n\u001B[32m      6\u001B[39m     \u001B[38;5;66;03m#     images=\"./assets/cir_query.png\",\u001B[39;00m\n\u001B[32m      7\u001B[39m     \u001B[38;5;66;03m#     q_or_c=\"q\",\u001B[39;00m\n\u001B[32m      8\u001B[39m     \u001B[38;5;66;03m#     task_instruction=\"Retrieve the target image that best meets the combined criteria by using both the provided image and the image retrieval instructions: \"\u001B[39;00m\n\u001B[32m      9\u001B[39m     \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     candidate_inputs = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata_process\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m[\u001B[49m\u001B[33;43m\"\"\"\u001B[39;49m\n\u001B[32m     14\u001B[39m \u001B[33;43m{\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mid\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:10093,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mimage_url\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/home/dkx/RAG4Ghart/bar/10093.png\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtype\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbar\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtheme\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mScience and Engineering\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtitle\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mData on R&D investment across cities\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdistribution\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrandom\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdisplay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mhorizontal\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mheader\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mCity,million USD(million USD)\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:[\u001B[39;49m\u001B[33;43m{\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mCrop category\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mRapeseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetric tons\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:20000},\u001B[39;49m\u001B[33;43m{\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mCrop category\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWheat\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetric tons\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:2694},\u001B[39;49m\u001B[33;43m{\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mCrop category\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mApple\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetric tons\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:2488},\u001B[39;49m\u001B[33;43m{\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mCrop category\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mSugarcane\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetric tons\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m:7573}]}\u001B[39;49m\n\u001B[32m     15\u001B[39m \u001B[33;43m\"\"\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m        \u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43mq_or_c\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mc\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# query_embs = model(**query_inputs, output_hidden_states=True)[:, -1, :]\u001B[39;00m\n\u001B[32m     22\u001B[39m     candi_embs = model(**candidate_inputs, output_hidden_states=\u001B[38;5;28;01mTrue\u001B[39;00m)[:, -\u001B[32m1\u001B[39m, :]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/BGE-VL-v1.5-zs/modeling_llavanext_for_embedding.py:336\u001B[39m, in \u001B[36mLLaVANextForEmbedding.data_process\u001B[39m\u001B[34m(self, images, text, q_or_c, task_instruction)\u001B[39m\n\u001B[32m    332\u001B[39m text_input = [\u001B[38;5;28mself\u001B[39m.prepare_text_input(_image, _text, q_or_c, task_instruction) \u001B[38;5;28;01mfor\u001B[39;00m _image, _text \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(images, text)]\n\u001B[32m    334\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m images \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    335\u001B[39m     \u001B[38;5;66;03m# 将每张image resize成512*512的大小\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m     images = [\u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_image\u001B[49m\u001B[43m)\u001B[49m.resize((\u001B[32m512\u001B[39m,\u001B[32m512\u001B[39m)).convert(\u001B[33m\"\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[32m    337\u001B[39m     inputs = \u001B[38;5;28mself\u001B[39m.processor(images=images, text=text_input, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m, padding=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    338\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/PIL/Image.py:3513\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(fp, mode, formats)\u001B[39m\n\u001B[32m   3511\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_path(fp):\n\u001B[32m   3512\u001B[39m     filename = os.fspath(fp)\n\u001B[32m-> \u001B[39m\u001B[32m3513\u001B[39m     fp = \u001B[43mbuiltins\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   3514\u001B[39m     exclusive_fp = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3515\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T01:27:32.591199Z",
     "start_time": "2025-07-21T01:27:32.445758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    # query = model.encode(\n",
    "    #     images = \"./assets/cir_query.png\",\n",
    "    #     text = \"Make the background dark, as if the camera has taken the photo at night\"\n",
    "    # )\n",
    "\n",
    "    candidates = model.encode(\n",
    "        text = \"\"\"\n",
    "{\"id\":10093,\"image_url\":\"/home/dkx/RAG4Ghart/bar/10093.png\",\"type\":\"bar\",\"theme\":\"Science and Engineering\",\"title\":\"Data on R&D investment across cities\",\"distribution\":\"random\",\"display\":\"horizontal\",\"header\":\"City,million USD(million USD)\",\"data\":[{\"Crop category\":\"Rapeseed\",\"metric tons\":20000},{\"Crop category\":\"Wheat\",\"metric tons\":2694},{\"Crop category\":\"Apple\",\"metric tons\":2488},{\"Crop category\":\"Sugarcane\",\"metric tons\":7573}]}\n",
    "\"\"\",\n",
    "        images = [\"/home/dukaixing/RAG4Ghart/Dataset-ZXQ/sample100/bar/png/21.png\"]\n",
    "    )\n",
    "\n",
    "    # scores = query @ candidates.T\n",
    "\n",
    "# print(query)\n",
    "print(candidates.shape)\n",
    "# print(scores)"
   ],
   "id": "64c26bb26fcada67",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLaVANextForEmbedding' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m      2\u001B[39m     \u001B[38;5;66;03m# query = model.encode(\u001B[39;00m\n\u001B[32m      3\u001B[39m     \u001B[38;5;66;03m#     images = \"./assets/cir_query.png\",\u001B[39;00m\n\u001B[32m      4\u001B[39m     \u001B[38;5;66;03m#     text = \"Make the background dark, as if the camera has taken the photo at night\"\u001B[39;00m\n\u001B[32m      5\u001B[39m     \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     candidates = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m(\n\u001B[32m      8\u001B[39m         text = \u001B[33m\"\"\"\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:10093,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mimage_url\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m/home/dkx/RAG4Ghart/bar/10093.png\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mbar\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtheme\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mScience and Engineering\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mData on R&D investment across cities\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdistribution\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mrandom\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdisplay\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mhorizontal\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mheader\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCity,million USD(million USD)\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:[\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRapeseed\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:20000},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mWheat\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:2694},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mApple\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:2488},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSugarcane\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:7573}]}        \u001B[39m\n\u001B[32m     10\u001B[39m \u001B[33m\"\"\"\u001B[39m,\n\u001B[32m     11\u001B[39m         images = [\u001B[33m\"\u001B[39m\u001B[33m/home/dukaixing/RAG4Ghart/Dataset-ZXQ/sample100/bar/png/21.png\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     12\u001B[39m     )\n\u001B[32m     14\u001B[39m     \u001B[38;5;66;03m# scores = query @ candidates.T\u001B[39;00m\n\u001B[32m     15\u001B[39m \n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# print(query)\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(candidates.shape)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001B[39m, in \u001B[36mModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   1938\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[32m   1939\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[32m-> \u001B[39m\u001B[32m1940\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[32m   1941\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m).\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m object has no attribute \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1942\u001B[39m )\n",
      "\u001B[31mAttributeError\u001B[39m: 'LLaVANextForEmbedding' object has no attribute 'encode'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:08:28.252338Z",
     "start_time": "2025-07-21T10:08:28.232357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "a = [torch.ones(2880, 4096)]\n",
    "b = torch.cat(a, dim=0)\n",
    "print(b.shape)"
   ],
   "id": "a8a20b5e877af810",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2880, 4096])\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
