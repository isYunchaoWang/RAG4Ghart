{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:05:41.703446Z",
     "start_time": "2025-07-21T10:05:36.029657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from modelscope import AutoModel\n",
    "from PIL import Image\n",
    "\n",
    "MODEL_NAME= \"/home/public/dkx/model/BAAI/BGE-VL-v1.5-zs\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model.eval()\n",
    "model.cuda()"
   ],
   "id": "aec64a6b3c13f922",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7315ccc4521149d5ae24ffb753806920"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LLaVANextForEmbedding(\n",
       "  (model): LlavaNextModel(\n",
       "    (vision_tower): CLIPVisionModel(\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(577, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): LlavaNextMultiModalProjector(\n",
       "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (act): GELUActivation()\n",
       "      (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (language_model): MistralModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): MistralRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:05:44.954987Z",
     "start_time": "2025-07-21T10:05:44.334060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    model.set_processor(MODEL_NAME)\n",
    "\n",
    "    # query_inputs = model.data_process(\n",
    "    #     text=\"Make the background dark, as if the camera has taken the photo at night\",\n",
    "    #     images=\"./assets/cir_query.png\",\n",
    "    #     q_or_c=\"q\",\n",
    "    #     task_instruction=\"Retrieve the target image that best meets the combined criteria by using both the provided image and the image retrieval instructions: \"\n",
    "    # )\n",
    "\n",
    "    candidate_inputs = model.data_process(\n",
    "        text=\"\"\"\n",
    "{\"id\":10093,\"image_url\":\"/home/dkx/RAG4Ghart/bar/10093.png\",\"type\":\"bar\",\"theme\":\"Science and Engineering\",\"title\":\"Data on R&D investment across cities\",\"distribution\":\"random\",\"display\":\"horizontal\",\"header\":\"City,million USD(million USD)\",\"data\":[{\"Crop category\":\"Rapeseed\",\"metric tons\":20000},{\"Crop category\":\"Wheat\",\"metric tons\":2694},{\"Crop category\":\"Apple\",\"metric tons\":2488},{\"Crop category\":\"Sugarcane\",\"metric tons\":7573}]}\n",
    "\"\"\",\n",
    "        images=[\"/home/dukaixing/RAG4Ghart/Dataset-ZXQ/sample100/bar/png/21.png\"],\n",
    "        q_or_c=\"c\",\n",
    "    )\n",
    "\n",
    "    # query_embs = model(**query_inputs, output_hidden_states=True)[:, -1, :]\n",
    "    candi_embs = model(**candidate_inputs, output_hidden_states=True)[:, -1, :]\n",
    "\n",
    "    # query_embs = torch.nn.functional.normalize(query_embs, dim=-1)\n",
    "    candi_embs = torch.nn.functional.normalize(candi_embs, dim=-1)\n",
    "\n",
    "    # scores = torch.matmul(query_embs, candi_embs.T)\n",
    "print(candi_embs.shape)"
   ],
   "id": "557704041e9d71ad",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:382: masked_scatter_size_check: block: [0,0,0], thread: [0,0,0] Assertion `totalElements <= srcSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m     13\u001B[39m     candidate_inputs = model.data_process(\n\u001B[32m     14\u001B[39m         text=\u001B[33m\"\"\"\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:10093,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mimage_url\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m/home/dkx/RAG4Ghart/bar/10093.png\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mbar\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtheme\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mScience and Engineering\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mData on R&D investment across cities\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdistribution\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mrandom\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdisplay\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mhorizontal\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mheader\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCity,million USD(million USD)\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:[\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRapeseed\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:20000},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mWheat\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:2694},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mApple\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:2488},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSugarcane\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:7573}]}\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     18\u001B[39m         q_or_c=\u001B[33m\"\u001B[39m\u001B[33mc\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     19\u001B[39m     )\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# query_embs = model(**query_inputs, output_hidden_states=True)[:, -1, :]\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     candi_embs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcandidate_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m[:, -\u001B[32m1\u001B[39m, :]\n\u001B[32m     24\u001B[39m     \u001B[38;5;66;03m# query_embs = torch.nn.functional.normalize(query_embs, dim=-1)\u001B[39;00m\n\u001B[32m     25\u001B[39m     candi_embs = torch.nn.functional.normalize(candi_embs, dim=-\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/BGE-VL-v1.5-zs/modeling_llavanext_for_embedding.py:246\u001B[39m, in \u001B[36mLLaVANextForEmbedding.forward\u001B[39m\u001B[34m(self, input_ids, pixel_values, image_sizes, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001B[39m\n\u001B[32m    243\u001B[39m         image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n\u001B[32m    244\u001B[39m         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n\u001B[32m--> \u001B[39m\u001B[32m246\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlanguage_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_logits_to_keep\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_logits_to_keep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:943\u001B[39m, in \u001B[36mcan_return_tuple.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    940\u001B[39m     set_attribute_for_modules(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m_is_top_level_module\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m943\u001B[39m     output = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    944\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_requested_to_return_tuple \u001B[38;5;129;01mor\u001B[39;00m (is_configured_to_return_tuple \u001B[38;5;129;01mand\u001B[39;00m is_top_level_module):\n\u001B[32m    945\u001B[39m         output = output.to_tuple()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:388\u001B[39m, in \u001B[36mMistralModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001B[39m\n\u001B[32m    386\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cache_position \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    387\u001B[39m     past_seen_tokens = past_key_values.get_seq_length() \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m     cache_position = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    389\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_seen_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_seen_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[32m    390\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    392\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m position_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    393\u001B[39m     position_ids = cache_position.unsqueeze(\u001B[32m0\u001B[39m)\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T01:27:32.591199Z",
     "start_time": "2025-07-21T01:27:32.445758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    # query = model.encode(\n",
    "    #     images = \"./assets/cir_query.png\",\n",
    "    #     text = \"Make the background dark, as if the camera has taken the photo at night\"\n",
    "    # )\n",
    "\n",
    "    candidates = model.encode(\n",
    "        text = \"\"\"\n",
    "{\"id\":10093,\"image_url\":\"/home/dkx/RAG4Ghart/bar/10093.png\",\"type\":\"bar\",\"theme\":\"Science and Engineering\",\"title\":\"Data on R&D investment across cities\",\"distribution\":\"random\",\"display\":\"horizontal\",\"header\":\"City,million USD(million USD)\",\"data\":[{\"Crop category\":\"Rapeseed\",\"metric tons\":20000},{\"Crop category\":\"Wheat\",\"metric tons\":2694},{\"Crop category\":\"Apple\",\"metric tons\":2488},{\"Crop category\":\"Sugarcane\",\"metric tons\":7573}]}\n",
    "\"\"\",\n",
    "        images = [\"/home/dukaixing/RAG4Ghart/Dataset-ZXQ/sample100/bar/png/21.png\"]\n",
    "    )\n",
    "\n",
    "    # scores = query @ candidates.T\n",
    "\n",
    "# print(query)\n",
    "print(candidates.shape)\n",
    "# print(scores)"
   ],
   "id": "64c26bb26fcada67",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLaVANextForEmbedding' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m      2\u001B[39m     \u001B[38;5;66;03m# query = model.encode(\u001B[39;00m\n\u001B[32m      3\u001B[39m     \u001B[38;5;66;03m#     images = \"./assets/cir_query.png\",\u001B[39;00m\n\u001B[32m      4\u001B[39m     \u001B[38;5;66;03m#     text = \"Make the background dark, as if the camera has taken the photo at night\"\u001B[39;00m\n\u001B[32m      5\u001B[39m     \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     candidates = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m(\n\u001B[32m      8\u001B[39m         text = \u001B[33m\"\"\"\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:10093,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mimage_url\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m/home/dkx/RAG4Ghart/bar/10093.png\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mbar\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtheme\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mScience and Engineering\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mData on R&D investment across cities\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdistribution\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mrandom\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdisplay\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mhorizontal\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mheader\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCity,million USD(million USD)\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:[\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRapeseed\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:20000},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mWheat\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:2694},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mApple\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:2488},\u001B[39m\u001B[33m{\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrop category\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSugarcane\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmetric tons\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m:7573}]}        \u001B[39m\n\u001B[32m     10\u001B[39m \u001B[33m\"\"\"\u001B[39m,\n\u001B[32m     11\u001B[39m         images = [\u001B[33m\"\u001B[39m\u001B[33m/home/dukaixing/RAG4Ghart/Dataset-ZXQ/sample100/bar/png/21.png\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     12\u001B[39m     )\n\u001B[32m     14\u001B[39m     \u001B[38;5;66;03m# scores = query @ candidates.T\u001B[39;00m\n\u001B[32m     15\u001B[39m \n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# print(query)\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(candidates.shape)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/RAG4Ghart/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001B[39m, in \u001B[36mModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   1938\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[32m   1939\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[32m-> \u001B[39m\u001B[32m1940\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[32m   1941\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m).\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m object has no attribute \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1942\u001B[39m )\n",
      "\u001B[31mAttributeError\u001B[39m: 'LLaVANextForEmbedding' object has no attribute 'encode'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:08:28.252338Z",
     "start_time": "2025-07-21T10:08:28.232357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "a = [torch.ones(2880, 4096)]\n",
    "b = torch.cat(a, dim=0)\n",
    "print(b.shape)"
   ],
   "id": "a8a20b5e877af810",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2880, 4096])\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
