{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T04:54:11.636848Z",
     "start_time": "2025-07-25T04:54:06.322668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from modelscope import AutoModel\n",
    "\n",
    "MODEL_NAME = \"/home/public/dkx/model/BAAI/BGE-VL-v1.5-zs\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model.eval()\n",
    "model.cuda()"
   ],
   "id": "ffda788fe691d54f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6088b36e56e940e7b51808a18396dc4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LLaVANextForEmbedding(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaNextMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-25T04:54:17.158700Z",
     "start_time": "2025-07-25T04:54:17.009201Z"
    }
   },
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(\n",
    "    uri=\"http://localhost:19530\"\n",
    ")\n",
    "\n",
    "client.load_collection(\"test\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T04:54:18.947874Z",
     "start_time": "2025-07-25T04:54:18.939497Z"
    }
   },
   "cell_type": "code",
   "source": "client.list_collections()",
   "id": "cc3f761109e087a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'bar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T04:54:24.044914Z",
     "start_time": "2025-07-25T04:54:23.427334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    model.set_processor(MODEL_NAME)\n",
    "\n",
    "    query_inputs = model.data_process(\n",
    "        text=\"\"\"\n",
    "Agriculture and Food Production, Production of Different Crops In An Area, metric tons, leptokurtic, horizontal\n",
    "Crop category,metric tons\n",
    "Rapeseed,20000\n",
    "Wheat,2694\n",
    "Apple,2488\n",
    "Sugarcane,7573\n",
    "\"\"\",\n",
    "        q_or_c=\"q\",\n",
    "        task_instruction=\"Recommend the most suitable chart with corresponding description for visualizing the information given by the provided text: \"\n",
    "    )\n",
    "\n",
    "    query_embs = model(**query_inputs, output_hidden_states=True)[:, -1, :]\n",
    "\n",
    "    query_embs = torch.nn.functional.normalize(query_embs, dim=-1)\n",
    "\n",
    "    print(len(query_embs.cpu().detach().tolist()[0]))"
   ],
   "id": "de8f789d7ff6df2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T05:37:09.378571Z",
     "start_time": "2025-07-25T05:37:09.355638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_search_results = client.search(\n",
    "    collection_name=\"test\",\n",
    "    anns_field=\"text_dense\",\n",
    "    data=query_embs.cpu().detach().tolist(),\n",
    "    limit=10,\n",
    "    search_params={\"metric_type\": \"IP\"},\n",
    "    output_fields=[\"id\", \"image_url\", \"data\", \"text_dense\", \"img_dense\"],  # specifies fields to be returned\n",
    ")\n",
    "\n",
    "for text_search_result in text_search_results[0]:\n",
    "    print(text_search_result[\"id\"])\n",
    "    print(text_search_result[\"distance\"])\n"
   ],
   "id": "5064a7aefef90471",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "267.5112609863281\n",
      "273\n",
      "242.0521240234375\n",
      "395\n",
      "231.0052490234375\n",
      "684\n",
      "224.5597686767578\n",
      "84\n",
      "220.9500732421875\n",
      "454\n",
      "219.68634033203125\n",
      "911\n",
      "217.21353149414062\n",
      "157\n",
      "214.20343017578125\n",
      "742\n",
      "211.1539764404297\n",
      "540\n",
      "210.75335693359375\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T05:37:29.821995Z",
     "start_time": "2025-07-25T05:37:29.791601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_search_results = client.search(\n",
    "    collection_name=\"test\",\n",
    "    anns_field=\"img_dense\",\n",
    "    data=query_embs.cpu().detach().tolist(),\n",
    "    limit=10,\n",
    "    search_params={\"metric_type\": \"IP\"},\n",
    "    output_fields=[\"id\", \"image_url\", \"data\", \"text_dense\", \"img_dense\"],  # specifies fields to be returned\n",
    ")\n",
    "\n",
    "for img_search_result in img_search_results[0]:\n",
    "    print(img_search_result[\"id\"])\n",
    "    print(img_search_result[\"distance\"])"
   ],
   "id": "2b7276601a290679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911\n",
      "111.25525665283203\n",
      "273\n",
      "111.1042251586914\n",
      "742\n",
      "108.90589904785156\n",
      "141\n",
      "108.09735107421875\n",
      "21\n",
      "106.78152465820312\n",
      "84\n",
      "104.885986328125\n",
      "454\n",
      "103.28755950927734\n",
      "540\n",
      "101.20672607421875\n",
      "395\n",
      "98.39408874511719\n",
      "157\n",
      "96.54594421386719\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T05:32:55.284975Z",
     "start_time": "2025-07-25T05:32:55.263411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymilvus import AnnSearchRequest\n",
    "\n",
    "# text semantic search (dense)\n",
    "request_1 = AnnSearchRequest(\n",
    "    data=query_embs.cpu().detach().tolist(),\n",
    "    anns_field=\"text_dense\",\n",
    "    param={\n",
    "        \"metric_type\": \"IP\"\n",
    "    },\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "# text-to-image search (multimodal)\n",
    "request_2 = AnnSearchRequest(\n",
    "    data=query_embs.cpu().detach().tolist(),\n",
    "    anns_field=\"img_dense\",\n",
    "    param={\n",
    "        \"metric_type\": \"IP\"\n",
    "    },\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "reqs = [request_1, request_2]"
   ],
   "id": "469abf8bcc2a6e21",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T05:32:58.123584Z",
     "start_time": "2025-07-25T05:32:58.118988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymilvus import RRFRanker, WeightedRanker\n",
    "\n",
    "rrf_ranker = RRFRanker(100)\n",
    "weighed_ranker = WeightedRanker(0, 1)"
   ],
   "id": "46840b9c5935d96c",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T05:41:21.659996Z",
     "start_time": "2025-07-25T05:41:21.647676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hybrid_search_results = client.hybrid_search(\n",
    "    collection_name=\"test\",  # target collection\n",
    "    reqs=reqs,\n",
    "    ranker=weighed_ranker,\n",
    "    limit=10,  # number of returned entities\n",
    "    output_fields=[\"id\", \"image_url\", \"metadata\"],  # specifies fields to be returned\n",
    ")"
   ],
   "id": "e714a0364568f06b",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T05:41:30.869917Z",
     "start_time": "2025-07-25T05:41:30.864458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for hybrid_search_result in hybrid_search_results[0]:\n",
    "    print(hybrid_search_result[\"id\"])\n",
    "    print(hybrid_search_result[\"metadata\"])"
   ],
   "id": "d7cce0d32299e2de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911\n",
      "{'distribution': ' step', 'display': ' vertical', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "273\n",
      "{'distribution': ' leptokurtic', 'display': ' vertical', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "742\n",
      "{'distribution': ' bimodal', 'display': ' vertical', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "141\n",
      "{'distribution': ' step', 'display': ' vertical', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "21\n",
      "{'distribution': ' leptokurtic', 'display': ' horizontal', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "84\n",
      "{'distribution': ' bimodal', 'display': ' vertical', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "454\n",
      "{'distribution': ' long_tail', 'display': ' vertical', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "540\n",
      "{'distribution': ' long_tail', 'display': ' horizontal', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "395\n",
      "{'distribution': ' step', 'display': ' vertical', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n",
      "157\n",
      "{'distribution': ' bimodal', 'display': ' vertical', 'header': ['Crop category', 'metric tons'], 'unit': ' metric tons'}\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T12:26:33.854729Z",
     "start_time": "2025-07-22T12:26:33.633266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    model.set_processor(MODEL_NAME)\n",
    "\n",
    "    query_inputs1 = model.data_process(\n",
    "        text=\"\"\"\n",
    "Agriculture and Food Production, Production of Different Crops In An Area, metric tons, leptokurtic, horizontal\n",
    "Crop category,metric tons\n",
    "Rapeseed,20000\n",
    "Wheat,2694\n",
    "Apple,2488\n",
    "Sugarcane,7573\n",
    "\"\"\",\n",
    "        q_or_c=\"q\",\n",
    "        task_instruction=\"Recommend the most suitable chart with corresponding description for visualizing the information given by the provided text: \"\n",
    "    )\n",
    "\n",
    "    query_embs1 = model(**query_inputs1, output_hidden_states=True)[:, -1, :]\n",
    "\n",
    "    query_embs1 = torch.nn.functional.normalize(query_embs1, dim=-1)\n",
    "\n",
    "    print(len(query_embs1.cpu().detach().tolist()[0]))"
   ],
   "id": "a97ff0aeeb9d64a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T12:26:36.343608Z",
     "start_time": "2025-07-22T12:26:36.331686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res1 = client.search(\n",
    "    collection_name=\"bar\",  # target collection\n",
    "    data=query_embs1.cpu().detach().tolist(),  # query vectors\n",
    "    limit=10,  # number of returned entities\n",
    "    output_fields=[\"id\", \"image_url\", \"data\"],  # specifies fields to be returned\n",
    ")\n",
    "\n",
    "for x in res1[0]:\n",
    "    print(x[\"id\"])\n",
    "    print(x[\"distance\"])"
   ],
   "id": "b3668fb7bb2fb95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911\n",
      "0.3492613136768341\n",
      "273\n",
      "0.34827539324760437\n",
      "141\n",
      "0.3397336006164551\n",
      "742\n",
      "0.3390982151031494\n",
      "84\n",
      "0.3294185698032379\n",
      "21\n",
      "0.327197790145874\n",
      "454\n",
      "0.3228279948234558\n",
      "540\n",
      "0.31269580125808716\n",
      "395\n",
      "0.3067818284034729\n",
      "157\n",
      "0.302957147359848\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
